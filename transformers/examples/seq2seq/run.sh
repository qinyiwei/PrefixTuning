 
The new price of the PlayStation Vita has been announced in Japan.


train:
python  finetune.py --model_name_or_path ~/ckp/mT5_multilingual_XLSum/ --output_dir ~/PrefixTuning_data/xlsum/mt5_base/amharic --data_dir ~/data/XLSum_input/individual/amharic/ --tuning_mode prefixtune --preseqlen 200 --do_train --gpus 1 --learning_rate 5e-05 --train_batch_size 4 --eval_batch_size 4 --num_train_epochs 3 --optim_prefix yes --preseqlen 200 --prefix_mode activation --format_mode cat --gradient_accumulation_steps 3 --learning_rate 5e-05 --weight_decay 0.0 --seed 101 --mid_dim 800 --use_dropout no --prefix_dropout 0.0  --max_source_length 512 --max_target_length 56 --val_max_target_length 142 --test_max_target_length 142 --cache_dir /home/yiweiq/.cache/huggingface/transformers --length_penalty 0.6 --max_length 512 --no_repeat_ngram_size 2 --use_self_prefix --use_encoder_prefix --use_cross_prefix

evaluate:
python  finetune.py --model_name_or_path ~/ckp/mT5_multilingual_XLSum/ --output_dir ~/PrefixTuning_data/xlsum/mt5_base/amharic_ref --data_dir ~/data/XLSum_input/individual/amharic/ --tuning_mode prefixtune --preseqlen 200 --do_predict --gpus 1 --learning_rate 5e-05 --train_batch_size 4 --eval_batch_size 4 --num_train_epochs 3 --optim_prefix yes --preseqlen 200 --prefix_mode activation --format_mode cat --gradient_accumulation_steps 3 --learning_rate 5e-05 --weight_decay 0.0 --seed 101 --mid_dim 800 --use_dropout no --prefix_dropout 0.0  --max_source_length 512 --max_target_length 56 --val_max_target_length 142 --test_max_target_length 142 --cache_dir /home/yiweiq/.cache/huggingface/transformers --length_penalty 0.6 --max_length 512 --no_repeat_ngram_size 2 





only evaluate:
python finetune.py --model_name_or_path ~/ckp/mT5_multilingual_XLSum/ --output_dir ~/PrefixTuning_data/xlsum/try/mt5_base_test_vanilla --data_dir ~/PrefixTuning_data/cnn_dm --tuning_mode prefixtune --preseqlen 200 --do_predict --gpus 2 --learning_rate 5e-05 --train_batch_size 8 --eval_batch_size 8  --optim_prefix yes --preseqlen 200 --prefix_mode activation --format_mode cat --gradient_accumulation_steps 3 --learning_rate 0 --weight_decay 0.0 --seed 101 --mid_dim 800 --use_dropout no --prefix_dropout 0.0  --max_source_length 512 --max_target_length 56 --val_max_target_length 142 --test_max_target_length 142 --cache_dir /home/yiweiq/.cache/huggingface/transformers --length_penalty 0.6 --max_length 512 --no_repeat_ngram_size 2


python  -m pdb  finetune.py --model_name_or_path ~/ckp/mT5_multilingual_XLSum/ --output_dir ~/PrefixTuning_data/xlsum/try/mt5_base --data_dir ~/PrefixTuning_data/cnn_dm/lowdata/ --tuning_mode prefixtune --preseqlen 200 --do_train --gpus 1 --learning_rate 5e-05 --train_batch_size 8 --eval_batch_size 8 --num_train_epochs 3 --optim_prefix yes --preseqlen 200 --prefix_mode activation --format_mode cat --gradient_accumulation_steps 3 --learning_rate 5e-05 --weight_decay 0.0 --seed 101 --mid_dim 800 --use_dropout no --prefix_dropout 0.0  --max_source_length 512 --max_target_length 56 --val_max_target_length 142 --test_max_target_length 142 --cache_dir /home/yiweiq/.cache/huggingface/transformers --length_penalty 0.6 --max_length 512 --no_repeat_ngram_size 2

python  -m pdb finetune.py --model_name_or_path t5-base --output_dir ~/PrefixTuning_data/xlsum/mt5-base/ --data_dir ~/PrefixTuning_data/cnn_dm --tuning_mode prefixtune --preseqlen 200 --do_train --gpus 1 --learning_rate 5e-05 --train_batch_size 16 --eval_batch_size 16 --num_train_epochs 3 --optim_prefix yes --preseqlen 200 --prefix_mode activation --format_mode cat --gradient_accumulation_steps 3 --learning_rate 5e-05 --weight_decay 0.0 --seed 101 --mid_dim 800 --use_dropout no --prefix_dropout 0.0  --max_source_length 512 --max_target_length 56 --val_max_target_length 142 --test_max_target_length 142  --fp16 --fp16_opt_level O1  --cache_dir /home/yiweiq/.cache/huggingface/transformers

python  -m pdb finetune.py --model_name_or_path facebook/bart-base --output_dir ~/PrefixTuning_data/cnndm_models/cnn_dmprefixtune_y_200_act_cat_b=48-e=30_d=0.0_u=no_lr=5e-05_w=0.0_s=101_r=n_m=800 --data_dir ~/PrefixTuning_data/cnn_dm --tuning_mode prefixtune --preseqlen 200 --do_train --gpus 1 --learning_rate 5e-05 --train_batch_size 16 --eval_batch_size 16 --num_train_epochs 3 --optim_prefix yes --preseqlen 200 --prefix_mode activation --format_mode cat --gradient_accumulation_steps 3 --learning_rate 5e-05 --weight_decay 0.0 --seed 101 --mid_dim 800 --use_dropout no --prefix_dropout 0.0  --max_source_length 512 --max_target_length 56 --val_max_target_length 142 --test_max_target_length 142  --fp16 --fp16_opt_level O1  --cache_dir /home/yiweiq/.cache/huggingface/transformers
